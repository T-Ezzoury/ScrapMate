<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tutorial &#8212; ScrapeMate 0.1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=01f34227"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="ScrapeMate Modules" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Link to this heading">¶</a></h1>
<p>Welcome to the ScrapeMate tutorial! This guide will walk you through the basics of using ScrapeMate to scrape and parse web content effectively.</p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#installation" id="id3">Installation</a></p></li>
<li><p><a class="reference internal" href="#fetching-and-parsing-content" id="id4">Fetching and Parsing Content</a></p></li>
<li><p><a class="reference internal" href="#saving-extracted-data" id="id5">Saving Extracted Data</a></p></li>
<li><p><a class="reference internal" href="#handling-common-use-cases" id="id6">Handling Common Use Cases</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id7">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#what-s-next" id="id8">What’s Next?</a></p></li>
<li><p><a class="reference internal" href="#additional-resources" id="id9">Additional Resources</a></p></li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>ScrapeMate is a lightweight Python package designed for quick and easy web scraping. Whether you’re collecting data for research, monitoring websites, or automating tasks, ScrapeMate provides a simple interface to get the job done.</p>
<p>In this tutorial, you’ll learn how to:</p>
<ol class="arabic simple">
<li><p><strong>Install ScrapeMate</strong></p></li>
<li><p><strong>Fetch HTML Content from a URL</strong></p></li>
<li><p><strong>Parse HTML to Extract Specific Elements</strong></p></li>
<li><p><strong>Save Extracted Data to CSV and JSON Formats</strong></p></li>
<li><p><strong>Handle Common Use Cases</strong></p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin, ensure that you have Python 3.8+ and Poetry installed on your system. If not, follow the [Prerequisites](#prerequisites) section to set them up.</p>
</div>
</section>
<section id="prerequisites">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Prerequisites</a><a class="headerlink" href="#prerequisites" title="Link to this heading">¶</a></h2>
<p>Before you begin, ensure you have the following:</p>
<ul class="simple">
<li><p><strong>Python 3.8+</strong> installed on your system.</p></li>
<li><p><strong>Poetry</strong> installed for dependency management. If you don’t have Poetry installed, you can install it by following the [Poetry Installation Guide](<a class="reference external" href="https://python-poetry.org/docs/#installation">https://python-poetry.org/docs/#installation</a>).</p></li>
</ul>
</section>
<section id="installation">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Installation</a><a class="headerlink" href="#installation" title="Link to this heading">¶</a></h2>
<p>To start using ScrapeMate, you’ll first need to install it along with its dependencies.</p>
<ol class="arabic">
<li><p><strong>Clone the Repository</strong></p>
<p>Begin by cloning the ScrapeMate repository to your local machine:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/your-username/ScrapeMate.git
<span class="nb">cd</span><span class="w"> </span>ScrapeMate
</pre></div>
</div>
</li>
<li><p><strong>Install Dependencies with Poetry</strong></p>
<p>Install the necessary dependencies using Poetry:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>poetry<span class="w"> </span>install
</pre></div>
</div>
<p>This command sets up a virtual environment and installs all required packages specified in the <cite>pyproject.toml</cite> file.</p>
</li>
<li><p><strong>Activate the Virtual Environment</strong></p>
<p>Activate the Poetry-managed virtual environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>poetry<span class="w"> </span>shell
</pre></div>
</div>
</li>
</ol>
</section>
<section id="fetching-and-parsing-content">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Fetching and Parsing Content</a><a class="headerlink" href="#fetching-and-parsing-content" title="Link to this heading">¶</a></h2>
<p>With ScrapeMate installed, you’re ready to fetch and parse web content.</p>
<ol class="arabic">
<li><p><strong>Fetch HTML Content from a URL</strong></p>
<p>Use the <cite>fetch_content</cite> function to retrieve HTML content from a specified URL.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrape_mate.scraper</span> <span class="kn">import</span> <span class="n">fetch_content</span>

<span class="c1"># Specify the URL you want to scrape</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://example.com&quot;</span>

<span class="c1"># Fetch the HTML content</span>
<span class="n">html_content</span> <span class="o">=</span> <span class="n">fetch_content</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">html_content</span><span class="p">)</span>  <span class="c1"># Prints the raw HTML content</span>
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>`fetch_content(url)`</strong>: Sends an HTTP GET request to the provided URL and returns the HTML content as a string.</p></li>
</ul>
</li>
<li><p><strong>Parse HTML to Extract Specific Elements</strong></p>
<p>Use the <cite>parse_html</cite> function to parse the fetched HTML and extract elements based on tag names and optional class names.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrape_mate.scraper</span> <span class="kn">import</span> <span class="n">parse_html</span>

<span class="c1"># Define the HTML tag you want to search for, e.g., &#39;div&#39;</span>
<span class="n">element_tag</span> <span class="o">=</span> <span class="s2">&quot;div&quot;</span>

<span class="c1"># (Optional) Define the class name to filter elements</span>
<span class="n">class_name</span> <span class="o">=</span> <span class="s2">&quot;example-class&quot;</span>

<span class="c1"># Parse the HTML content to find matching elements</span>
<span class="n">elements</span> <span class="o">=</span> <span class="n">parse_html</span><span class="p">(</span><span class="n">html_content</span><span class="p">,</span> <span class="n">element_tag</span><span class="p">,</span> <span class="n">class_name</span><span class="p">)</span>

<span class="c1"># Iterate through the extracted elements and print their text content</span>
<span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">elements</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">element</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>`parse_html(html_content, element_tag, class_name)`</strong>: Parses the HTML content to find all elements matching the specified tag and class name. Returns a list of BeautifulSoup Tag objects.</p></li>
</ul>
</li>
</ol>
</section>
<section id="saving-extracted-data">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Saving Extracted Data</a><a class="headerlink" href="#saving-extracted-data" title="Link to this heading">¶</a></h2>
<p>After extracting the desired elements, you can save the data in various formats like CSV and JSON.</p>
<ol class="arabic">
<li><p><strong>Save Data to CSV</strong></p>
<p>Use the <cite>save_to_csv</cite> function to save the extracted data to a CSV file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrape_mate.file_saver</span> <span class="kn">import</span> <span class="n">save_to_csv</span>

<span class="c1"># Prepare data as a list of rows (each row is a list of values)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s2">&quot;Title&quot;</span><span class="p">,</span> <span class="s2">&quot;Description&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Example Title 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Description for title 1&quot;</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Example Title 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Description for title 2&quot;</span><span class="p">],</span>
    <span class="c1"># Add more rows as needed</span>
<span class="p">]</span>

<span class="c1"># Specify the filename for the CSV</span>
<span class="n">csv_filename</span> <span class="o">=</span> <span class="s2">&quot;extracted_data.csv&quot;</span>

<span class="c1"># Save the data to CSV</span>
<span class="n">save_to_csv</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">csv_filename</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data successfully saved to </span><span class="si">{</span><span class="n">csv_filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>`save_to_csv(data, filename)`</strong>: Writes the provided data to a CSV file with the specified filename.</p></li>
</ul>
</li>
<li><p><strong>Save Data to JSON</strong></p>
<p>Use the <cite>save_to_json</cite> function to save the extracted data to a JSON file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrape_mate.file_saver</span> <span class="kn">import</span> <span class="n">save_to_json</span>

<span class="c1"># Prepare data as a dictionary or any JSON-serializable object</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;titles&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Example Title 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Example Title 2&quot;</span><span class="p">],</span>
    <span class="s2">&quot;descriptions&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Description for title 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Description for title 2&quot;</span><span class="p">],</span>
    <span class="c1"># Add more key-value pairs as needed</span>
<span class="p">}</span>

<span class="c1"># Specify the filename for the JSON</span>
<span class="n">json_filename</span> <span class="o">=</span> <span class="s2">&quot;extracted_data.json&quot;</span>

<span class="c1"># Save the data to JSON</span>
<span class="n">save_to_json</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">json_filename</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data successfully saved to </span><span class="si">{</span><span class="n">json_filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>`save_to_json(data, filename)`</strong>: Serializes the provided data to a JSON file with the specified filename.</p></li>
</ul>
</li>
</ol>
</section>
<section id="handling-common-use-cases">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Handling Common Use Cases</a><a class="headerlink" href="#handling-common-use-cases" title="Link to this heading">¶</a></h2>
<p>ScrapeMate is versatile and can handle various web scraping scenarios. Below are a couple of common use cases to get you started.</p>
<ol class="arabic">
<li><p><strong>Scraping Multiple Pages</strong></p>
<p>Suppose you want to scrape data from multiple pages of a website. You can loop through a list of URLs and aggregate the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrape_mate.scraper</span> <span class="kn">import</span> <span class="n">fetch_content</span><span class="p">,</span> <span class="n">parse_html</span>
<span class="kn">from</span> <span class="nn">scrape_mate.file_saver</span> <span class="kn">import</span> <span class="n">save_to_json</span>

<span class="c1"># List of URLs to scrape</span>
<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;https://example.com/page1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://example.com/page2&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://example.com/page3&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Initialize a list to hold all extracted data</span>
<span class="n">all_data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
    <span class="n">html</span> <span class="o">=</span> <span class="n">fetch_content</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">elements</span> <span class="o">=</span> <span class="n">parse_html</span><span class="p">(</span><span class="n">html</span><span class="p">,</span> <span class="s2">&quot;div&quot;</span><span class="p">,</span> <span class="s2">&quot;example-class&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">elements</span><span class="p">:</span>
        <span class="n">title</span> <span class="o">=</span> <span class="n">element</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">&quot;h2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">description</span> <span class="o">=</span> <span class="n">element</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="n">all_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="n">description</span><span class="p">})</span>

<span class="c1"># Save the aggregated data to JSON</span>
<span class="n">save_to_json</span><span class="p">(</span><span class="n">all_data</span><span class="p">,</span> <span class="s2">&quot;all_extracted_data.json&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All data successfully saved to all_extracted_data.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Automating the Scraping Process</strong></p>
<p>You can automate the scraping process by scheduling your script to run at regular intervals using tools like <strong>cron</strong> (on Unix-based systems) or <strong>Task Scheduler</strong> (on Windows).</p>
<p><strong>Example Cron Job (Runs Daily at Midnight):</strong></p>
<ol class="arabic">
<li><p><strong>Open the cron table:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>crontab<span class="w"> </span>-e
</pre></div>
</div>
</li>
<li><p><strong>Add the following line to schedule the scraping script:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span>*<span class="w"> </span>*<span class="w"> </span>*<span class="w"> </span>/path/to/your/virtualenv/bin/python<span class="w"> </span>/path/to/your/project/ScrapeMate/scrape_script.py
</pre></div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>`0 0 * * *`</strong>: Specifies the job to run daily at midnight.</p></li>
<li><p><strong>`/path/to/your/virtualenv/bin/python`</strong>: Path to the Python interpreter in your virtual environment.</p></li>
<li><p><strong>`/path/to/your/project/ScrapeMate/scrape_script.py`</strong>: Path to your scraping script.</p></li>
</ul>
</li>
</ol>
<p><strong>Note:</strong> Ensure your script handles potential errors and logs its activities for easier troubleshooting.</p>
</li>
</ol>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>Congratulations! You’ve successfully completed the ScrapeMate tutorial. You’ve learned how to install ScrapeMate, fetch and parse web content, save extracted data, and handle common scraping scenarios. With these skills, you’re well-equipped to leverage ScrapeMate for your web scraping projects.</p>
</section>
<section id="what-s-next">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">What’s Next?</a><a class="headerlink" href="#what-s-next" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><strong>Explore Advanced Features:</strong> Dive deeper into ScrapeMate’s capabilities, such as handling authentication, managing sessions, or integrating with databases.</p></li>
<li><p><strong>Contribute to ScrapeMate:</strong> If you have ideas for improvements or new features, consider contributing to the project.</p></li>
<li><p><strong>Share Your Projects:</strong> Showcase what you’ve built using ScrapeMate, whether it’s data analysis, reporting, or automation tools.</p></li>
</ul>
</section>
<section id="additional-resources">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Additional Resources</a><a class="headerlink" href="#additional-resources" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><strong>ScrapeMate Documentation:</strong> <a class="reference external" href="https://github.com/your-username/ScrapeMate/docs/build/html/index.html">Link to your reference documentation</a></p></li>
<li><p><strong>Sphinx Documentation:</strong> <a class="reference external" href="https://www.sphinx-doc.org/en/master/">https://www.sphinx-doc.org/en/master/</a></p></li>
<li><p><strong>BeautifulSoup Documentation:</strong> <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p></li>
<li><p><strong>Requests Documentation:</strong> <a class="reference external" href="https://requests.readthedocs.io/en/latest/">https://requests.readthedocs.io/en/latest/</a></p></li>
<li><p><strong>Poetry Documentation:</strong> <a class="reference external" href="https://python-poetry.org/docs/">https://python-poetry.org/docs/</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For further assistance or questions, feel free to reach out through the project’s repository.</p>
</div>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">ScrapeMate</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="modules.html">ScrapeMate Modules</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fetching-and-parsing-content">Fetching and Parsing Content</a></li>
<li class="toctree-l2"><a class="reference internal" href="#saving-extracted-data">Saving Extracted Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#handling-common-use-cases">Handling Common Use Cases</a></li>
<li class="toctree-l2"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-next">What’s Next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-resources">Additional Resources</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">ScrapeMate Modules</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Taha EZ-ZOURY.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/tutorial.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>